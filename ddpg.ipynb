{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- env = gym.make(\"Pendulum-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DDPG Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-01-06 09:09:39,215] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "View more on the tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# reproducible\n",
    "np.random.seed(1)\n",
    "tf.set_random_seed(1)\n",
    "#####################  hyper parameters  ####################\n",
    "\n",
    "\n",
    "MAX_EP_STEPS = 500\n",
    "LR_A = 0.001    # learning rate for actor\n",
    "LR_C = 0.002    # learning rate for critic\n",
    "GAMMA = 0.9     # reward discount\n",
    "TAU = 0.01      # soft replacement\n",
    "MEMORY_CAPACITY = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "###############################  DDPG  ####################################\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, a_dim, s_dim, a_bound,):\n",
    "        self.memory = np.zeros((MEMORY_CAPACITY, s_dim * 2 + a_dim + 1), dtype=np.float32)\n",
    "        self.pointer = 0\n",
    "        self.sess = tf.Session()\n",
    "        self.var = 3.0\n",
    "        # self.a_replace_counter, self.c_replace_counter = 0, 0\n",
    "\n",
    "        self.a_dim, self.s_dim, self.a_bound = a_dim, s_dim, a_bound,\n",
    "        self.S = tf.placeholder(tf.float32, [None, s_dim], 's')\n",
    "        self.S_ = tf.placeholder(tf.float32, [None, s_dim], 's_')\n",
    "        self.R = tf.placeholder(tf.float32, [None, 1], 'r')\n",
    "\n",
    "        with tf.variable_scope('Actor'):\n",
    "            self.a = self.build_a_nn(self.S, scope='eval', trainable=True)\n",
    "            a_ = self.build_a_nn(self.S_, scope='target', trainable=False)\n",
    "        with tf.variable_scope('Critic'):\n",
    "            # assign self.a = a in memory when calculating q for td_error,\n",
    "            # otherwise the self.a is from Actor when updating Actor\n",
    "            q = self.build_c_nn(self.S, self.a, scope='eval', trainable=True)\n",
    "            q_ = self.build_c_nn(self.S_, a_, scope='target', trainable=False)\n",
    "\n",
    "        # networks parameters\n",
    "        self.ae_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/eval')\n",
    "        self.at_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target')\n",
    "        self.ce_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/eval')\n",
    "        self.ct_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target')\n",
    "\n",
    "        # target net replacement\n",
    "        self.soft_replace = [[tf.assign(at, (1-TAU)*at+TAU*ae), tf.assign(ct, (1-TAU)*ct+TAU*ce)]\n",
    "            for at, ae, ct, ce in zip(self.at_params, self.ae_params, self.ct_params, self.ce_params)]\n",
    "\n",
    "        q_target = self.R + GAMMA * q_\n",
    "        # in the feed_dic for the td_error, the self.a should change to actions in memory\n",
    "        td_error = tf.losses.mean_squared_error(labels=(self.R + GAMMA * q_), predictions=q)\n",
    "        self.ctrain = tf.train.AdamOptimizer(LR_C).minimize(td_error, name=\"adam-ink\", var_list = self.ce_params)\n",
    "\n",
    "        a_loss = - tf.reduce_mean(q)    # maximize the q\n",
    "        self.atrain = tf.train.AdamOptimizer(LR_A).minimize(a_loss, var_list=self.ae_params)\n",
    "\n",
    "        tf.summary.FileWriter(\"logs/\", self.sess.graph)\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "       \n",
    "\n",
    "    def choose_action(self, s):\n",
    "        a = self.sess.run(self.a, {self.S: s[np.newaxis, :]})[0]\n",
    "        a = np.clip(np.random.normal(a, self.var), -2, 2)\n",
    "        \n",
    "        return a\n",
    "\n",
    "    def learn(self):\n",
    "        # soft target replacement\n",
    "        self.sess.run(self.soft_replace)\n",
    "\n",
    "        indices = np.random.choice(MEMORY_CAPACITY, size=BATCH_SIZE)\n",
    "        bt = self.memory[indices, :]\n",
    "        bs = bt[:, :self.s_dim]\n",
    "        ba = bt[:, self.s_dim: self.s_dim + self.a_dim]\n",
    "        br = bt[:, -self.s_dim - 1: -self.s_dim]\n",
    "        bs_ = bt[:, -self.s_dim:]\n",
    "\n",
    "        self.sess.run(self.atrain, {self.S: bs})\n",
    "        self.sess.run(self.ctrain, {self.S: bs, self.a: ba, self.R: br, self.S_: bs_})\n",
    "\n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        trans = np.hstack((s,a,[r],s_))\n",
    "        \n",
    "        index = self.pointer % MEMORY_CAPACITY\n",
    "        self.memory[index, :] = trans\n",
    "        self.pointer += 1\n",
    "\n",
    "        if self.pointer > MEMORY_CAPACITY:\n",
    "            # print(4557)\n",
    "            self.var *= 0.99995\n",
    "            self.learn()\n",
    "    # def _build_a(self, s, scope, trainable):\n",
    "    #     with tf.variable_scope(scope):\n",
    "    #         net = tf.layers.dense(s, 30, activation=tf.nn.tanh, name='l1', trainable=trainable)\n",
    "    #         a = tf.layers.dense(net, self.a_dim, activation=tf.nn.tanh, name='a', trainable=trainable)\n",
    "    #         print(\"a: \", a)\n",
    "    #         return tf.multiply(a, self.a_bound, name='scaled_a')\n",
    "    def build_a_nn(self, s, scope, trainable):\n",
    "        # Actor DPG\n",
    "        with tf.variable_scope(scope):\n",
    "            l1 = tf.layers.dense(s, 30, activation = tf.nn.tanh, name = 'l1', trainable = trainable)\n",
    "            a = tf.layers.dense(l1, self.a_dim, activation = tf.nn.tanh, name = 'a', trainable = trainable)     \n",
    "            return tf.multiply(a, self.a_bound, name = \"scaled_a\")  \n",
    "    # def _build_c(self, s, a, scope, trainable):\n",
    "    #     with tf.variable_scope(scope):\n",
    "    #         n_l1 = 30\n",
    "    #         w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable=trainable)\n",
    "    #         w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable=trainable)\n",
    "    #         b1 = tf.get_variable('b1', [1, n_l1], trainable=trainable)\n",
    "    #         net = tf.nn.tanh(tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1)\n",
    "    #         return tf.layers.dense(net, 1, trainable=trainable)  # Q(s,a)\n",
    "    def build_c_nn(self, s, a, scope, trainable):\n",
    "        # Critic Q-leaning\n",
    "        with tf.variable_scope(scope):\n",
    "            n_l1 = 30\n",
    "            w1_s = tf.get_variable('w1_s', [self.s_dim, n_l1], trainable = trainable)\n",
    "            w1_a = tf.get_variable('w1_a', [self.a_dim, n_l1], trainable = trainable)\n",
    "            b1 = tf.get_variable('b1', [1, n_l1], trainable = trainable)\n",
    "            net = tf.nn.tanh( tf.matmul(s, w1_s) + tf.matmul(a, w1_a) + b1 )\n",
    "\n",
    "            q = tf.layers.dense(net, 1, trainable = trainable)\n",
    "            return q\n",
    "\n",
    "    \n",
    "###############################  training  ####################################\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "env = env.unwrapped\n",
    "env.seed(1)\n",
    "\n",
    "s_dim = env.observation_space.shape[0]\n",
    "a_dim = env.action_space.shape[0]\n",
    "a_bound = env.action_space.high\n",
    "\n",
    "# s_dim = env.state_dim\n",
    "# a_dim = env.action_dim\n",
    "# a_bound = 0.2\n",
    "\n",
    "ddpg = DDPG(a_dim, s_dim, a_bound)\n",
    "\n",
    "r_save = []\n",
    "\n",
    "# var = 3  # control exploration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPISODES = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:  0\n",
      "Episode: 0  Reward: -3945 Explore: 3.00\n",
      "i:  1\n",
      "Episode: 1  Reward: -3833 Explore: 3.00\n",
      "i:  2\n",
      "Episode: 2  Reward: -3713 Explore: 3.00\n",
      "i:  3\n",
      "Episode: 3  Reward: -4105 Explore: 3.00\n",
      "i:  4\n",
      "Episode: 4  Reward: -3805 Explore: 3.00\n",
      "i:  5\n",
      "Episode: 5  Reward: -4173 Explore: 3.00\n",
      "i:  6\n",
      "Episode: 6  Reward: -3660 Explore: 3.00\n",
      "i:  7\n",
      "Episode: 7  Reward: -4362 Explore: 3.00\n",
      "i:  8\n",
      "Episode: 8  Reward: -3571 Explore: 3.00\n",
      "i:  9\n",
      "Episode: 9  Reward: -4017 Explore: 3.00\n",
      "i:  10\n",
      "Episode: 10  Reward: -3612 Explore: 3.00\n",
      "i:  11\n",
      "Episode: 11  Reward: -3744 Explore: 3.00\n",
      "i:  12\n",
      "Episode: 12  Reward: -4160 Explore: 3.00\n",
      "i:  13\n",
      "Episode: 13  Reward: -4033 Explore: 3.00\n",
      "i:  14\n",
      "Episode: 14  Reward: -3771 Explore: 3.00\n",
      "i:  15\n",
      "Episode: 15  Reward: -4229 Explore: 3.00\n",
      "i:  16\n",
      "Episode: 16  Reward: -3953 Explore: 3.00\n",
      "i:  17\n",
      "Episode: 17  Reward: -3563 Explore: 3.00\n",
      "i:  18\n",
      "Episode: 18  Reward: -4070 Explore: 3.00\n",
      "i:  19\n",
      "Episode: 19  Reward: -4145 Explore: 3.00\n",
      "i:  20\n",
      "Episode: 20  Reward: -3219 Explore: 2.93\n",
      "i:  21\n",
      "Episode: 21  Reward: -3462 Explore: 2.85\n",
      "i:  22\n",
      "Episode: 22  Reward: -3271 Explore: 2.78\n",
      "i:  23\n",
      "Episode: 23  Reward: -3223 Explore: 2.71\n",
      "i:  24\n",
      "Episode: 24  Reward: -3079 Explore: 2.65\n",
      "i:  25\n",
      "Episode: 25  Reward: -3127 Explore: 2.58\n",
      "i:  26\n",
      "Episode: 26  Reward: -3360 Explore: 2.52\n",
      "i:  27\n",
      "Episode: 27  Reward: -3447 Explore: 2.46\n",
      "i:  28\n",
      "Episode: 28  Reward: -3379 Explore: 2.40\n",
      "i:  29\n",
      "Episode: 29  Reward: -3510 Explore: 2.34\n",
      "i:  30\n",
      "Episode: 30  Reward: -3429 Explore: 2.28\n",
      "i:  31\n",
      "Episode: 31  Reward: -3102 Explore: 2.22\n",
      "i:  32\n",
      "Episode: 32  Reward: -3433 Explore: 2.17\n",
      "i:  33\n",
      "Episode: 33  Reward: -2641 Explore: 2.11\n",
      "i:  34\n",
      "Episode: 34  Reward: -2234 Explore: 2.06\n",
      "i:  35\n",
      "Episode: 35  Reward: -2790 Explore: 2.01\n",
      "i:  36\n",
      "Episode: 36  Reward: -2906 Explore: 1.96\n",
      "i:  37\n",
      "Episode: 37  Reward: -3416 Explore: 1.91\n",
      "i:  38\n",
      "Episode: 38  Reward: -3312 Explore: 1.87\n",
      "i:  39\n",
      "Episode: 39  Reward: -2330 Explore: 1.82\n",
      "i:  40\n",
      "Episode: 40  Reward: -2403 Explore: 1.77\n",
      "i:  41\n",
      "Episode: 41  Reward: -2370 Explore: 1.73\n",
      "i:  42\n",
      "Episode: 42  Reward: -1541 Explore: 1.69\n",
      "i:  43\n",
      "Episode: 43  Reward: -3245 Explore: 1.65\n",
      "i:  44\n",
      "Episode: 44  Reward: -3139 Explore: 1.61\n",
      "i:  45\n",
      "Episode: 45  Reward: -3425 Explore: 1.57\n",
      "i:  46\n",
      "Episode: 46  Reward: -288 Explore: 1.53\n",
      "i:  47\n",
      "Episode: 47  Reward: -2553 Explore: 1.49\n",
      "i:  48\n",
      "Episode: 48  Reward: -1642 Explore: 1.45\n",
      "i:  49\n",
      "Episode: 49  Reward: -1048 Explore: 1.42\n",
      "i:  50\n",
      "Episode: 50  Reward: -897 Explore: 1.38\n",
      "i:  51\n",
      "Episode: 51  Reward: -391 Explore: 1.35\n",
      "i:  52\n",
      "Episode: 52  Reward: -649 Explore: 1.31\n",
      "i:  53\n",
      "Episode: 53  Reward: -533 Explore: 1.28\n",
      "i:  54\n",
      "Episode: 54  Reward: -542 Explore: 1.25\n",
      "i:  55\n",
      "Episode: 55  Reward: -126 Explore: 1.22\n",
      "i:  56\n",
      "Episode: 56  Reward: -2755 Explore: 1.19\n",
      "i:  57\n",
      "Episode: 57  Reward: -129 Explore: 1.16\n",
      "i:  58\n",
      "Episode: 58  Reward: -129 Explore: 1.13\n",
      "i:  59\n",
      "Episode: 59  Reward: -526 Explore: 1.10\n",
      "i:  60\n",
      "Episode: 60  Reward: -266 Explore: 1.08\n",
      "i:  61\n",
      "Episode: 61  Reward: -132 Explore: 1.05\n",
      "i:  62\n",
      "Episode: 62  Reward: -400 Explore: 1.02\n",
      "i:  63\n",
      "Episode: 63  Reward: -254 Explore: 1.00\n",
      "i:  64\n",
      "Episode: 64  Reward: -130 Explore: 0.97\n",
      "i:  65\n",
      "Episode: 65  Reward: -126 Explore: 0.95\n",
      "i:  66\n",
      "Episode: 66  Reward: -127 Explore: 0.93\n",
      "i:  67\n",
      "Episode: 67  Reward: -391 Explore: 0.90\n",
      "i:  68\n",
      "Episode: 68  Reward: -378 Explore: 0.88\n",
      "i:  69\n",
      "Episode: 69  Reward: -402 Explore: 0.86\n",
      "i:  70\n",
      "Episode: 70  Reward: -255 Explore: 0.84\n",
      "i:  71\n",
      "Episode: 71  Reward: -401 Explore: 0.82\n",
      "i:  72\n",
      "Episode: 72  Reward: -126 Explore: 0.80\n",
      "i:  73\n",
      "Episode: 73  Reward: -123 Explore: 0.78\n",
      "i:  74\n",
      "Episode: 74  Reward: -261 Explore: 0.76\n",
      "i:  75\n",
      "Episode: 75  Reward: -268 Explore: 0.74\n",
      "i:  76\n",
      "Episode: 76  Reward: -634 Explore: 0.72\n",
      "i:  77\n",
      "Episode: 77  Reward: -132 Explore: 0.70\n",
      "i:  78\n",
      "Episode: 78  Reward: -378 Explore: 0.69\n",
      "i:  79\n",
      "Episode: 79  Reward: -252 Explore: 0.67\n",
      "i:  80\n",
      "Episode: 80  Reward: -492 Explore: 0.65\n",
      "i:  81\n",
      "Episode: 81  Reward: -3 Explore: 0.64\n",
      "i:  82\n",
      "Episode: 82  Reward: -259 Explore: 0.62\n",
      "i:  83\n",
      "Episode: 83  Reward: -366 Explore: 0.61\n",
      "i:  84\n",
      "Episode: 84  Reward: -128 Explore: 0.59\n",
      "i:  85\n",
      "Episode: 85  Reward: -121 Explore: 0.58\n",
      "i:  86\n",
      "Episode: 86  Reward: -134 Explore: 0.56\n",
      "i:  87\n",
      "Episode: 87  Reward: -122 Explore: 0.55\n",
      "i:  88\n",
      "Episode: 88  Reward: -132 Explore: 0.53\n",
      "i:  89\n",
      "Episode: 89  Reward: -130 Explore: 0.52\n",
      "i:  90\n",
      "Episode: 90  Reward: -273 Explore: 0.51\n",
      "i:  91\n",
      "Episode: 91  Reward: -130 Explore: 0.50\n",
      "i:  92\n",
      "Episode: 92  Reward: -135 Explore: 0.48\n",
      "i:  93\n",
      "Episode: 93  Reward: -129 Explore: 0.47\n",
      "i:  94\n",
      "Episode: 94  Reward: -3 Explore: 0.46\n",
      "i:  95\n",
      "Episode: 95  Reward: -126 Explore: 0.45\n",
      "i:  96\n",
      "Episode: 96  Reward: -130 Explore: 0.44\n",
      "i:  97\n",
      "Episode: 97  Reward: -128 Explore: 0.43\n",
      "i:  98\n",
      "Episode: 98  Reward: -130 Explore: 0.42\n",
      "i:  99\n",
      "Episode: 99  Reward: -378 Explore: 0.41\n",
      "i:  100\n",
      "Episode: 100  Reward: -251 Explore: 0.40\n",
      "i:  101\n",
      "Episode: 101  Reward: -134 Explore: 0.39\n",
      "i:  102\n",
      "Episode: 102  Reward: -378 Explore: 0.38\n",
      "i:  103\n",
      "Episode: 103  Reward: -2 Explore: 0.37\n",
      "i:  104\n",
      "Episode: 104  Reward: -255 Explore: 0.36\n",
      "i:  105\n",
      "Episode: 105  Reward: -2 Explore: 0.35\n",
      "i:  106\n",
      "Episode: 106  Reward: -4 Explore: 0.34\n",
      "i:  107\n",
      "Episode: 107  Reward: -123 Explore: 0.33\n",
      "i:  108\n",
      "Episode: 108  Reward: -2807 Explore: 0.32\n",
      "i:  109\n",
      "Episode: 109  Reward: -126 Explore: 0.32\n",
      "i:  110\n",
      "Episode: 110  Reward: -127 Explore: 0.31\n",
      "i:  111\n",
      "Episode: 111  Reward: -120 Explore: 0.30\n",
      "i:  112\n",
      "Episode: 112  Reward: -362 Explore: 0.29\n",
      "i:  113\n",
      "Episode: 113  Reward: -129 Explore: 0.29\n",
      "i:  114\n",
      "Episode: 114  Reward: -1 Explore: 0.28\n",
      "i:  115\n",
      "Episode: 115  Reward: -371 Explore: 0.27\n",
      "i:  116\n",
      "Episode: 116  Reward: -238 Explore: 0.27\n",
      "i:  117\n",
      "Episode: 117  Reward: -242 Explore: 0.26\n",
      "i:  118\n",
      "Episode: 118  Reward: -1 Explore: 0.25\n",
      "i:  119\n",
      "Episode: 119  Reward: -267 Explore: 0.25\n",
      "i:  120\n",
      "Episode: 120  Reward: -255 Explore: 0.24\n",
      "i:  121\n",
      "Episode: 121  Reward: -126 Explore: 0.23\n",
      "i:  122\n",
      "Episode: 122  Reward: -236 Explore: 0.23\n",
      "i:  123\n",
      "Episode: 123  Reward: 0 Explore: 0.22\n",
      "i:  124\n",
      "Episode: 124  Reward: -125 Explore: 0.22\n",
      "i:  125\n",
      "Episode: 125  Reward: -125 Explore: 0.21\n",
      "i:  126\n",
      "Episode: 126  Reward: -128 Explore: 0.21\n",
      "i:  127\n",
      "Episode: 127  Reward: -256 Explore: 0.20\n",
      "i:  128\n",
      "Episode: 128  Reward: -1 Explore: 0.20\n",
      "i:  129\n",
      "Episode: 129  Reward: -1 Explore: 0.19\n",
      "i:  130\n",
      "Episode: 130  Reward: -1 Explore: 0.19\n",
      "i:  131\n",
      "Episode: 131  Reward: -254 Explore: 0.18\n",
      "i:  132\n",
      "Episode: 132  Reward: -123 Explore: 0.18\n",
      "i:  133\n",
      "Episode: 133  Reward: -129 Explore: 0.17\n",
      "i:  134\n",
      "Episode: 134  Reward: -373 Explore: 0.17\n",
      "i:  135\n",
      "Episode: 135  Reward: -131 Explore: 0.17\n",
      "i:  136\n",
      "Episode: 136  Reward: -268 Explore: 0.16\n",
      "i:  137\n",
      "Episode: 137  Reward: -122 Explore: 0.16\n",
      "i:  138\n",
      "Episode: 138  Reward: -282 Explore: 0.15\n",
      "i:  139\n",
      "Episode: 139  Reward: -238 Explore: 0.15\n",
      "i:  140\n",
      "Episode: 140  Reward: -126 Explore: 0.15\n",
      "i:  141\n",
      "Episode: 141  Reward: -386 Explore: 0.14\n",
      "i:  142\n",
      "Episode: 142  Reward: -126 Explore: 0.14\n",
      "i:  143\n",
      "Episode: 143  Reward: -521 Explore: 0.14\n",
      "i:  144\n",
      "Episode: 144  Reward: -118 Explore: 0.13\n",
      "i:  145\n",
      "Episode: 145  Reward: -255 Explore: 0.13\n",
      "i:  146\n",
      "Episode: 146  Reward: -3210 Explore: 0.13\n",
      "i:  147\n",
      "Episode: 147  Reward: -1037 Explore: 0.12\n",
      "i:  148\n",
      "Episode: 148  Reward: -241 Explore: 0.12\n",
      "i:  149\n",
      "Episode: 149  Reward: -127 Explore: 0.12\n",
      "i:  150\n",
      "Episode: 150  Reward: -134 Explore: 0.11\n",
      "i:  151\n",
      "Episode: 151  Reward: -364 Explore: 0.11\n",
      "i:  152\n",
      "Episode: 152  Reward: -254 Explore: 0.11\n",
      "i:  153\n",
      "Episode: 153  Reward: -1 Explore: 0.11\n",
      "i:  154\n",
      "Episode: 154  Reward: -512 Explore: 0.10\n",
      "i:  155\n",
      "Episode: 155  Reward: -127 Explore: 0.10\n",
      "i:  156\n",
      "Episode: 156  Reward: -507 Explore: 0.10\n",
      "i:  157\n",
      "Episode: 157  Reward: -472 Explore: 0.10\n",
      "i:  158\n",
      "Episode: 158  Reward: -130 Explore: 0.09\n",
      "i:  159\n",
      "Episode: 159  Reward: -561 Explore: 0.09\n",
      "i:  160\n",
      "Episode: 160  Reward: -129 Explore: 0.09\n",
      "i:  161\n",
      "Episode: 161  Reward: -277 Explore: 0.09\n",
      "i:  162\n",
      "Episode: 162  Reward: -128 Explore: 0.08\n",
      "i:  163\n",
      "Episode: 163  Reward: -5 Explore: 0.08\n",
      "i:  164\n",
      "Episode: 164  Reward: -262 Explore: 0.08\n",
      "i:  165\n",
      "Episode: 165  Reward: -132 Explore: 0.08\n",
      "i:  166\n",
      "Episode: 166  Reward: -417 Explore: 0.08\n",
      "i:  167\n",
      "Episode: 167  Reward: -266 Explore: 0.07\n",
      "i:  168\n",
      "Episode: 168  Reward: -268 Explore: 0.07\n",
      "i:  169\n",
      "Episode: 169  Reward: -131 Explore: 0.07\n",
      "i:  170\n",
      "Episode: 170  Reward: -275 Explore: 0.07\n",
      "i:  171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 171  Reward: -308 Explore: 0.07\n",
      "i:  172\n",
      "Episode: 172  Reward: -1443 Explore: 0.07\n",
      "i:  173\n",
      "Episode: 173  Reward: -130 Explore: 0.06\n",
      "i:  174\n",
      "Episode: 174  Reward: -129 Explore: 0.06\n",
      "i:  175\n",
      "Episode: 175  Reward: -128 Explore: 0.06\n",
      "i:  176\n",
      "Episode: 176  Reward: -263 Explore: 0.06\n",
      "i:  177\n",
      "Episode: 177  Reward: -133 Explore: 0.06\n",
      "i:  178\n",
      "Episode: 178  Reward: -130 Explore: 0.06\n",
      "i:  179\n",
      "Episode: 179  Reward: -126 Explore: 0.05\n",
      "i:  180\n",
      "Episode: 180  Reward: -259 Explore: 0.05\n",
      "i:  181\n",
      "Episode: 181  Reward: -388 Explore: 0.05\n",
      "i:  182\n",
      "Episode: 182  Reward: -126 Explore: 0.05\n",
      "i:  183\n",
      "Episode: 183  Reward: -124 Explore: 0.05\n",
      "i:  184\n",
      "Episode: 184  Reward: -237 Explore: 0.05\n",
      "i:  185\n",
      "Episode: 185  Reward: -124 Explore: 0.05\n",
      "i:  186\n",
      "Episode: 186  Reward: -121 Explore: 0.05\n",
      "i:  187\n",
      "Episode: 187  Reward: -134 Explore: 0.04\n",
      "i:  188\n",
      "Episode: 188  Reward: -123 Explore: 0.04\n",
      "i:  189\n",
      "Episode: 189  Reward: -123 Explore: 0.04\n",
      "i:  190\n",
      "Episode: 190  Reward: -246 Explore: 0.04\n",
      "i:  191\n",
      "Episode: 191  Reward: -466 Explore: 0.04\n",
      "i:  192\n",
      "Episode: 192  Reward: -257 Explore: 0.04\n",
      "i:  193\n",
      "Episode: 193  Reward: -1 Explore: 0.04\n",
      "i:  194\n",
      "Episode: 194  Reward: -352 Explore: 0.04\n",
      "i:  195\n",
      "Episode: 195  Reward: -261 Explore: 0.04\n",
      "i:  196\n",
      "Episode: 196  Reward: -120 Explore: 0.04\n",
      "i:  197\n",
      "Episode: 197  Reward: 0 Explore: 0.04\n",
      "i:  198\n",
      "Episode: 198  Reward: -118 Explore: 0.03\n",
      "i:  199\n",
      "Episode: 199  Reward: -250 Explore: 0.03\n",
      "i:  200\n",
      "Episode: 200  Reward: -122 Explore: 0.03\n",
      "i:  201\n",
      "Episode: 201  Reward: 0 Explore: 0.03\n",
      "i:  202\n",
      "Episode: 202  Reward: -126 Explore: 0.03\n",
      "i:  203\n",
      "Episode: 203  Reward: -258 Explore: 0.03\n",
      "i:  204\n",
      "Episode: 204  Reward: -126 Explore: 0.03\n",
      "i:  205\n",
      "Episode: 205  Reward: -132 Explore: 0.03\n",
      "i:  206\n",
      "Episode: 206  Reward: -399 Explore: 0.03\n",
      "i:  207\n",
      "Episode: 207  Reward: -549 Explore: 0.03\n",
      "i:  208\n",
      "Episode: 208  Reward: -131 Explore: 0.03\n",
      "i:  209\n",
      "Episode: 209  Reward: -3 Explore: 0.03\n",
      "i:  210\n",
      "Episode: 210  Reward: -133 Explore: 0.03\n",
      "i:  211\n",
      "Episode: 211  Reward: -3 Explore: 0.02\n",
      "i:  212\n",
      "Episode: 212  Reward: -133 Explore: 0.02\n",
      "i:  213\n",
      "Episode: 213  Reward: -271 Explore: 0.02\n",
      "i:  214\n",
      "Episode: 214  Reward: -305 Explore: 0.02\n",
      "i:  215\n",
      "Episode: 215  Reward: -266 Explore: 0.02\n",
      "i:  216\n",
      "Episode: 216  Reward: -258 Explore: 0.02\n",
      "i:  217\n",
      "Episode: 217  Reward: -126 Explore: 0.02\n",
      "i:  218\n",
      "Episode: 218  Reward: -128 Explore: 0.02\n",
      "i:  219\n",
      "Episode: 219  Reward: -670 Explore: 0.02\n",
      "i:  220\n",
      "Episode: 220  Reward: -406 Explore: 0.02\n",
      "i:  221\n",
      "Episode: 221  Reward: -129 Explore: 0.02\n",
      "i:  222\n",
      "Episode: 222  Reward: -257 Explore: 0.02\n",
      "i:  223\n",
      "Episode: 223  Reward: -1 Explore: 0.02\n",
      "i:  224\n",
      "Episode: 224  Reward: -271 Explore: 0.02\n",
      "i:  225\n",
      "Episode: 225  Reward: -1 Explore: 0.02\n",
      "i:  226\n",
      "Episode: 226  Reward: -125 Explore: 0.02\n",
      "i:  227\n",
      "Episode: 227  Reward: -126 Explore: 0.02\n",
      "i:  228\n",
      "Episode: 228  Reward: -1 Explore: 0.02\n",
      "i:  229\n",
      "Episode: 229  Reward: -130 Explore: 0.02\n",
      "i:  230\n",
      "Episode: 230  Reward: 0 Explore: 0.02\n",
      "i:  231\n",
      "Episode: 231  Reward: -250 Explore: 0.01\n",
      "i:  232\n",
      "Episode: 232  Reward: -372 Explore: 0.01\n",
      "i:  233\n",
      "Episode: 233  Reward: -131 Explore: 0.01\n",
      "i:  234\n",
      "Episode: 234  Reward: -249 Explore: 0.01\n",
      "i:  235\n",
      "Episode: 235  Reward: -130 Explore: 0.01\n",
      "i:  236\n",
      "Episode: 236  Reward: -276 Explore: 0.01\n",
      "i:  237\n",
      "Episode: 237  Reward: -1 Explore: 0.01\n",
      "i:  238\n",
      "Episode: 238  Reward: -1816 Explore: 0.01\n",
      "i:  239\n",
      "Episode: 239  Reward: -128 Explore: 0.01\n",
      "i:  240\n",
      "Episode: 240  Reward: -130 Explore: 0.01\n",
      "i:  241\n",
      "Episode: 241  Reward: -260 Explore: 0.01\n",
      "i:  242\n",
      "Episode: 242  Reward: -135 Explore: 0.01\n",
      "i:  243\n",
      "Episode: 243  Reward: -1 Explore: 0.01\n",
      "i:  244\n",
      "Episode: 244  Reward: -1 Explore: 0.01\n",
      "i:  245\n",
      "Episode: 245  Reward: -126 Explore: 0.01\n",
      "i:  246\n",
      "Episode: 246  Reward: 0 Explore: 0.01\n",
      "i:  247\n",
      "Episode: 247  Reward: -260 Explore: 0.01\n",
      "i:  248\n",
      "Episode: 248  Reward: 0 Explore: 0.01\n",
      "i:  249\n",
      "Episode: 249  Reward: -270 Explore: 0.01\n",
      "i:  250\n",
      "Episode: 250  Reward: -249 Explore: 0.01\n",
      "i:  251\n",
      "Episode: 251  Reward: -390 Explore: 0.01\n",
      "i:  252\n",
      "Episode: 252  Reward: -268 Explore: 0.01\n",
      "i:  253\n",
      "Episode: 253  Reward: -379 Explore: 0.01\n",
      "i:  254\n",
      "Episode: 254  Reward: -128 Explore: 0.01\n",
      "i:  255\n",
      "Episode: 255  Reward: -120 Explore: 0.01\n",
      "i:  256\n",
      "Episode: 256  Reward: -135 Explore: 0.01\n",
      "i:  257\n",
      "Episode: 257  Reward: -261 Explore: 0.01\n",
      "i:  258\n",
      "Episode: 258  Reward: -130 Explore: 0.01\n",
      "i:  259\n",
      "Episode: 259  Reward: -267 Explore: 0.01\n",
      "i:  260\n",
      "Episode: 260  Reward: -132 Explore: 0.01\n",
      "i:  261\n",
      "Episode: 261  Reward: -3 Explore: 0.01\n",
      "i:  262\n",
      "Episode: 262  Reward: -123 Explore: 0.01\n",
      "i:  263\n",
      "Episode: 263  Reward: -127 Explore: 0.01\n",
      "i:  264\n",
      "Episode: 264  Reward: -123 Explore: 0.01\n",
      "i:  265\n",
      "Episode: 265  Reward: 0 Explore: 0.01\n",
      "i:  266\n",
      "Episode: 266  Reward: -377 Explore: 0.01\n",
      "i:  267\n",
      "Episode: 267  Reward: -238 Explore: 0.01\n",
      "i:  268\n",
      "Episode: 268  Reward: -131 Explore: 0.01\n",
      "i:  269\n",
      "Episode: 269  Reward: -126 Explore: 0.01\n",
      "i:  270\n",
      "Episode: 270  Reward: 0 Explore: 0.01\n",
      "i:  271\n",
      "Episode: 271  Reward: -270 Explore: 0.01\n",
      "i:  272\n",
      "Episode: 272  Reward: 0 Explore: 0.01\n",
      "i:  273\n",
      "Episode: 273  Reward: -120 Explore: 0.01\n",
      "i:  274\n",
      "Episode: 274  Reward: -126 Explore: 0.01\n",
      "i:  275\n",
      "Episode: 275  Reward: -122 Explore: 0.00\n",
      "i:  276\n",
      "Episode: 276  Reward: -127 Explore: 0.00\n",
      "i:  277\n",
      "Episode: 277  Reward: -235 Explore: 0.00\n",
      "i:  278\n",
      "Episode: 278  Reward: -2 Explore: 0.00\n",
      "i:  279\n",
      "Episode: 279  Reward: -256 Explore: 0.00\n",
      "i:  280\n",
      "Episode: 280  Reward: -619 Explore: 0.00\n",
      "i:  281\n",
      "Episode: 281  Reward: -252 Explore: 0.00\n",
      "i:  282\n",
      "Episode: 282  Reward: -521 Explore: 0.00\n",
      "i:  283\n",
      "Episode: 283  Reward: -411 Explore: 0.00\n",
      "i:  284\n",
      "Episode: 284  Reward: -403 Explore: 0.00\n",
      "i:  285\n",
      "Episode: 285  Reward: -527 Explore: 0.00\n",
      "i:  286\n",
      "Episode: 286  Reward: -406 Explore: 0.00\n",
      "i:  287\n",
      "Episode: 287  Reward: -1800 Explore: 0.00\n",
      "i:  288\n",
      "Episode: 288  Reward: -127 Explore: 0.00\n",
      "i:  289\n",
      "Episode: 289  Reward: -10 Explore: 0.00\n",
      "i:  290\n",
      "Episode: 290  Reward: -124 Explore: 0.00\n",
      "i:  291\n",
      "Episode: 291  Reward: -6 Explore: 0.00\n",
      "i:  292\n",
      "Episode: 292  Reward: -145 Explore: 0.00\n",
      "i:  293\n",
      "Episode: 293  Reward: -305 Explore: 0.00\n",
      "i:  294\n",
      "Episode: 294  Reward: -139 Explore: 0.00\n",
      "i:  295\n",
      "Episode: 295  Reward: -269 Explore: 0.00\n",
      "i:  296\n",
      "Episode: 296  Reward: -134 Explore: 0.00\n",
      "i:  297\n",
      "Episode: 297  Reward: -898 Explore: 0.00\n",
      "i:  298\n",
      "Episode: 298  Reward: 0 Explore: 0.00\n",
      "i:  299\n",
      "Episode: 299  Reward: -377 Explore: 0.00\n"
     ]
    }
   ],
   "source": [
    "for i in range(MAX_EPISODES):\n",
    "    print(\"i: \", i)\n",
    "    s = env.reset()\n",
    "    ep_reward = 0\n",
    "    for j in range(MAX_EP_STEPS):\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        # Add exploration noise\n",
    "        a = ddpg.choose_action(s)\n",
    "        # a = np.clip(np.random.normal(a, var), -2, 2)    # add randomness to action selection for exploration\n",
    "        # print(\"a: \", a)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # print(\"r: \", r)\n",
    "        ddpg.store_transition(s, a, r, s_)\n",
    "\n",
    "        # if ddpg.pointer > MEMORY_CAPACITY:\n",
    "        #     ddpg.var *= .9995    # decay the action randomness\n",
    "        #     ddpg.learn()\n",
    "\n",
    "        s = s_\n",
    "        ep_reward += r\n",
    "\n",
    "        \n",
    "\n",
    "        if j == MAX_EP_STEPS-1:\n",
    "            r_save.append(ep_reward)\n",
    "            print('Episode:', i, ' Reward: %i' % int(ep_reward), 'Explore: %.2f' % ddpg.var, )\n",
    "            # if ep_reward > -300:RENDER = True\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_reward = r_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD8CAYAAACCRVh7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nERkxSBARkdH/B2GBcq1LbHQiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f31582320f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# ax.plot([1, 2, 3, 4], [10, 20, 25, 30], color='lightblue', linewidth=3) # plot line\n",
    "ax.scatter(range(len(all_reward)), all_reward, color='green', marker='^') # plot points\n",
    "# ax.set_xlim(0.5, 4.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
